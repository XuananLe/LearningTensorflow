{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_1:0\", shape=(2, 3), dtype=float32)\n",
      "(2, 3)\n",
      "<dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.constant([[1., 2., 3.],\n",
    "                 [4., 5., 6.]])\n",
    "\n",
    "print(x) # This is called a tensor\n",
    "print(x.shape)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow **IS NOT** using the GPU\n"
     ]
    }
   ],
   "source": [
    "# Config the tensorflow to use GPU\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "      print(\"TensorFlow **IS** using the GPU\")\n",
    "else:\n",
    "  print(\"TensorFlow **IS NOT** using the GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m   y \u001b[39m=\u001b[39m f(x)\n\u001b[1;32m     10\u001b[0m g_x \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient(y, x)  \u001b[39m# g(x) = dy/dx\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[39mprint\u001b[39m(g_x\u001b[39m.\u001b[39;49mnumpy())\n",
      "File \u001b[0;32m~/Documents/LearningTensorflow/venv/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:430\u001b[0m, in \u001b[0;36mTensor.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mT\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mastype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mravel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtranspose\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mreshape\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mclip\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msize\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    422\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtolist\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[1;32m    423\u001b[0m   \u001b[39m# TODO(wangpeng): Export the enable_numpy_behavior knob\u001b[39;00m\n\u001b[1;32m    424\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m    425\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m    426\u001b[0m \u001b[39m    If you are looking for numpy-related methods, please run the following:\u001b[39m\n\u001b[1;32m    427\u001b[0m \u001b[39m    from tensorflow.python.ops.numpy_ops import np_config\u001b[39m\n\u001b[1;32m    428\u001b[0m \u001b[39m    np_config.enable_numpy_behavior()\u001b[39m\n\u001b[1;32m    429\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"\u001b[39m)\n\u001b[0;32m--> 430\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(1.0)\n",
    "\n",
    "def f(x):\n",
    "  y = x**2 + 2*x - 5\n",
    "  return y\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  y = f(x)\n",
    "\n",
    "g_x = tape.gradient(y, x)  # g(x) = dy/dx\n",
    "print(g_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tensorflow is a framework for deep learning. It is a symbolic math library, and is also used for machine learning applications such as neural networks. One of the main thing it uses is the Computational Graph. The main thing it helps with is that we can split the graph into many parts and use our hardware across many servers to calculate it.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"Images/ComGraph.png\" alt=\"Alt text describing the image\" width=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-20T13:31:40.206563725Z",
     "start_time": "2023-08-20T13:31:40.158924925Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-21 07:11:27.605140: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-21 07:11:27.620450: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-21 07:11:27.739341: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-21 07:11:27.740340: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-21 07:11:28.633624: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "tf.compat.v1.disable_eager_execution() # need to disable eager in TF2.\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-20T13:31:40.207003306Z",
     "start_time": "2023-08-20T13:31:40.206054710Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-21 07:11:29.431873: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Đây là các bước có thể xây dựng một computational graph\n",
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "f = x*x*y + y + 2\n",
    "# Tuy nhiên, để có thể chạy nó thì cần phải có một session. Session ở đây được định nghĩa là một cách để chạy một computational graph.\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-20T13:31:40.207768956Z",
     "start_time": "2023-08-20T13:31:40.206330383Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x1 = tf.Variable(5)\n",
    "print(x1.graph is tf.compat.v1.get_default_graph())\n",
    "# Ngay khi một node được tạo ra, nó sẽ được thêm vào default graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-20T13:31:40.208044186Z",
     "start_time": "2023-08-20T13:31:40.206474721Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Trong trường hợp chúng ta muốn có nhiều graph thì chúng ta có thể tạo ra một graph mới \n",
    "graph = tf.Graph() # Graph của chúng ta\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(10)\n",
    "print(x2.graph is graph)\n",
    "print(x2.graph is tf.compat.v1.get_default_graph())\n",
    "tf.compat.v1.reset_default_graph()\n",
    "# Ở trong tensorflow nếu như muốn dùng 1 hàm mà nó chỉ có ở ver trước nhưng k có ở ver này thì chúng ta có thể sử dụng hàm compat.v1 và compat.v2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-20T13:31:40.273532473Z",
     "start_time": "2023-08-20T13:31:40.206746552Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# Vòng đời của 1 node trong graph\n",
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x * 3\n",
    "with tf.compat.v1.Session() as _:\n",
    "    print(y.eval())  # 10\n",
    "    print(z.eval())  # 15\n",
    "# TensorFlow sẽ tính toán lại các node mà nó cần để tính toán node mà chúng ta cần. Nó sẽ tự động dò được các biến phụ thuộc. Tuy nhiên, nó sẽ không dùng lại kết quả của các node mà nó đã tính toán trước đó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-20T13:31:40.273863929Z",
     "start_time": "2023-08-20T13:31:40.249779931Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# Nếu như chúng ta không muốn tính toán lại 2 lần kết quả thì chúng ta \n",
    "with tf.compat.v1.Session() as _:\n",
    "    y_val, z_val = _.run([y, z])\n",
    "    # Tính toán y_val và z_val trong 1 lượt chạy \n",
    "    print(y_val)  # 10\n",
    "    print(z_val)  # 15  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-20T13:31:40.274015619Z",
     "start_time": "2023-08-20T13:31:40.249908416Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20640 8\n"
     ]
    }
   ],
   "source": [
    "# Tf ops có thể từ các input trả về các output.Nếu như là biến thì nó gọi là source ops. Input và ouput đều là các multidimensional arrays aka TENSOR. Vì thế chúng ta có cái tên là Tensorflow\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "print(m, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.7152519e+01]\n",
      " [ 4.3609828e-01]\n",
      " [ 9.3990080e-03]\n",
      " [-1.0659942e-01]\n",
      " [ 6.4186364e-01]\n",
      " [-4.0663608e-06]\n",
      " [-3.7808868e-03]\n",
      " [-4.2343181e-01]\n",
      " [-4.3694982e-01]]\n"
     ]
    }
   ],
   "source": [
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "    \n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.linalg.inv(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "\n",
    "with tf.compat.v1.Session() as _:\n",
    "    theta_value = theta.eval()\n",
    "    print(theta_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Chúng ta sẽ sử dụng autodiff thay vì đi tính tay mấy cái đạo hàm. 1 Ví dụ có thể thấy là exp(exp(exp(x))). Hình như là cần tận 9 lần để có thể tính toán được đạo hàm của nó. May cho chúng ta thì autodiff ở trong tf ops có thể giải quyết cho chúng ta điều này. Nó có thể tự động tính cho mình gradient.\n",
    "```` python \n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "\n",
    "Nó sẽ tự động tính gradient dựa theo mse và theta. theta ở đây đang là list các biến mà chúng ta muốn tính gradient theo. \n",
    "Tóm lại thứ chúng ta cần là 2 thứ\n",
    "optimize dùng để : Tối ưu hàm mất mát (loss function) thông qua thuật toán tối ưu hóa nào đó (như Gradient Descent). Trong TensorFlow, bạn thường sử dụng một optimizer như tf.train.GradientDescentOptimizer hoặc các optimizer khác như AdamOptimizer, MomentumOptimizer, v.v.\n",
    "Các optimizer này có phương thức minimize() giúp tự động tính toán gradient và cập nhật giá trị của các biến theo gradient đó.\n",
    "\n",
    "\n",
    "training_op dùng để: Đại diện cho thao tác (operation) cập nhật giá trị của các biến trong quá trình huấn luyện. Khi bạn chạy sess.run(training_op), nó sẽ thực hiện một bước tối ưu (ví dụ: một bước Gradient Descent) để cập nhật giá trị của các biến dựa trên gradient của hàm mất mát.\n",
    "````\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-20T14:15:46.325038513Z",
     "start_time": "2023-08-20T14:15:46.209969882Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  100.0 x_value:  10.0\n",
      "Epoch:  100 Loss:  0.0017876907 x_value:  0.04228109\n",
      "Epoch:  200 Loss:  4.2703396e-10 x_value:  -2.06648e-05\n",
      "Epoch:  300 Loss:  1.5444532e-12 x_value:  -1.2427603e-06\n",
      "Epoch:  400 Loss:  4.4233583e-17 x_value:  -6.650833e-09\n",
      "Epoch:  500 Loss:  3.048242e-23 x_value:  -5.521089e-12\n",
      "Epoch:  600 Loss:  2.092928e-26 x_value:  1.4466955e-13\n",
      "Epoch:  700 Loss:  9.697628e-31 x_value:  9.847654e-16\n",
      "Epoch:  800 Loss:  3.4687296e-36 x_value:  1.8624526e-18\n",
      "Epoch:  900 Loss:  0.0 x_value:  -1.5366877e-20\n",
      "\n",
      "\n",
      "Best Epoch:  826 Best x_value:  1.809685e-20 Smallest Loss:  0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "x = tf.Variable(10.0, trainable=True)\n",
    "# Định nghĩa hàm loss\n",
    "loss = tf.square(x)\n",
    "\n",
    "# Định nghĩa optimizer là Gradient Descent\n",
    "optimizer = tf.compat.v1.train.MomentumOptimizer (learning_rate=learning_rate,\n",
    "                                                  momentum=0.9)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "values = []\n",
    "smallest_loss = float('inf')  # set to infinity initially\n",
    "best_epoch = -1\n",
    "best_x_value = None\n",
    "\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    for epoch in range(n_epochs):\n",
    "        # rất bực ở chỗ là mỗi lần chạy thì nó phải sess.run\n",
    "        _, loss_value, x_value = sess.run([training_op, loss, x])\n",
    "        if loss_value < smallest_loss:\n",
    "            smallest_loss = loss_value\n",
    "            best_epoch = epoch\n",
    "            best_x_value = x_value\n",
    "        values.append((epoch, loss_value, x_value))\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch: \", epoch, \"Loss: \", loss_value, \"x_value: \", x_value)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"Best Epoch: \", best_epoch, \"Best x_value: \", best_x_value, \"Smallest Loss: \", smallest_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Feeding data cho thuật toán. Chúng ta có thể dùng placeholder để feed data cho thuật toán. Placeholder là một node mà nó sẽ không tính toán được. Nó chỉ là một node mà chúng ta có thể feed data vào. Chúng ta có thể feed data vào placeholder bằng cách sử dụng feed_dict. Feed_dict là một dictionary mà key là placeholdeBr và value là data mà chúng ta muốn feed vào."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-20T14:21:10.095115746Z",
     "start_time": "2023-08-20T14:21:10.089657207Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = tf.compat.v1.placeholder(tf.float32, shape=(None, 3))\n",
    "B = A + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-20T14:35:11.990177195Z",
     "start_time": "2023-08-20T14:35:11.924304408Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
