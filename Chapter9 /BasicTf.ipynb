{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tensorflow is a framework for deep learning. It is a symbolic math library, and is also used for machine learning applications such as neural networks. One of the main thing it uses is the Computational Graph. The main thing it helps with is that we can split the graph into many parts and use our hardware across many servers to calculate it.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"Images/ComGraph.png\" alt=\"Alt text describing the image\" width=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-20T13:31:40.206563725Z",
     "start_time": "2023-08-20T13:31:40.158924925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "tf.compat.v1.disable_eager_execution() # need to disable eager in TF2.\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-20T13:31:40.207003306Z",
     "start_time": "2023-08-20T13:31:40.206054710Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Đây là các bước có thể xây dựng một computational graph\n",
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "f = x*x*y + y + 2\n",
    "# Tuy nhiên, để có thể chạy nó thì cần phải có một session. Session ở đây được định nghĩa là một cách để chạy một computational graph.\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-20T13:31:40.207768956Z",
     "start_time": "2023-08-20T13:31:40.206330383Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x1 = tf.Variable(5)\n",
    "print(x1.graph is tf.compat.v1.get_default_graph())\n",
    "# Ngay khi một node được tạo ra, nó sẽ được thêm vào default graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-20T13:31:40.208044186Z",
     "start_time": "2023-08-20T13:31:40.206474721Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Trong trường hợp chúng ta muốn có nhiều graph thì chúng ta có thể tạo ra một graph mới \n",
    "graph = tf.Graph() # Graph của chúng ta\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(10)\n",
    "print(x2.graph is graph)\n",
    "print(x2.graph is tf.compat.v1.get_default_graph())\n",
    "tf.compat.v1.reset_default_graph()\n",
    "# Ở trong tensorflow nếu như muốn dùng 1 hàm mà nó chỉ có ở ver trước nhưng k có ở ver này thì chúng ta có thể sử dụng hàm compat.v1 và compat.v2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-20T13:31:40.273532473Z",
     "start_time": "2023-08-20T13:31:40.206746552Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# Vòng đời của 1 node trong graph\n",
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x * 3\n",
    "with tf.compat.v1.Session() as _:\n",
    "    print(y.eval())  # 10\n",
    "    print(z.eval())  # 15\n",
    "# TensorFlow sẽ tính toán lại các node mà nó cần để tính toán node mà chúng ta cần. Nó sẽ tự động dò được các biến phụ thuộc. Tuy nhiên, nó sẽ không dùng lại kết quả của các node mà nó đã tính toán trước đó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-20T13:31:40.273863929Z",
     "start_time": "2023-08-20T13:31:40.249779931Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# Nếu như chúng ta không muốn tính toán lại 2 lần kết quả thì chúng ta \n",
    "with tf.compat.v1.Session() as _:\n",
    "    y_val, z_val = _.run([y, z])\n",
    "    # Tính toán y_val và z_val trong 1 lượt chạy \n",
    "    print(y_val)  # 10\n",
    "    print(z_val)  # 15  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-20T13:31:40.274015619Z",
     "start_time": "2023-08-20T13:31:40.249908416Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20640 8\n"
     ]
    }
   ],
   "source": [
    "# Tf ops có thể từ các input trả về các output.Nếu như là biến thì nó gọi là source ops. Input và ouput đều là các multidimensional arrays aka TENSOR. Vì thế chúng ta có cái tên là Tensorflow\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "print(m, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "    \n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.linalg.inv(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "\n",
    "with tf.compat.v1.Session() as _:\n",
    "    theta_value = theta.eval()\n",
    "    print(theta_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Chúng ta sẽ sử dụng autodiff thay vì đi tính tay mấy cái đạo hàm. 1 Ví dụ có thể thấy là exp(exp(exp(x))). Hình như là cần tận 9 lần để có thể tính toán được đạo hàm của nó. May cho chúng ta thì autodiff ở trong tf ops có thể giải quyết cho chúng ta điều này. Nó có thể tự động tính cho mình gradient.\n",
    "```` python \n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "\n",
    "Nó sẽ tự động tính gradient dựa theo mse và theta. theta ở đây đang là list các biến mà chúng ta muốn tính gradient theo. \n",
    "Tóm lại thứ chúng ta cần là 2 thứ\n",
    "optimize dùng để : Tối ưu hàm mất mát (loss function) thông qua thuật toán tối ưu hóa nào đó (như Gradient Descent). Trong TensorFlow, bạn thường sử dụng một optimizer như tf.train.GradientDescentOptimizer hoặc các optimizer khác như AdamOptimizer, MomentumOptimizer, v.v.\n",
    "Các optimizer này có phương thức minimize() giúp tự động tính toán gradient và cập nhật giá trị của các biến theo gradient đó.\n",
    "\n",
    "\n",
    "training_op dùng để: Đại diện cho thao tác (operation) cập nhật giá trị của các biến trong quá trình huấn luyện. Khi bạn chạy sess.run(training_op), nó sẽ thực hiện một bước tối ưu (ví dụ: một bước Gradient Descent) để cập nhật giá trị của các biến dựa trên gradient của hàm mất mát.\n",
    "````\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-20T14:15:46.325038513Z",
     "start_time": "2023-08-20T14:15:46.209969882Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  100.0 x_value:  10.0\n",
      "Epoch:  100 Loss:  0.0017876907 x_value:  0.04228109\n",
      "Epoch:  200 Loss:  4.2703396e-10 x_value:  -2.06648e-05\n",
      "Epoch:  300 Loss:  1.5444532e-12 x_value:  -1.2427603e-06\n",
      "Epoch:  400 Loss:  4.4233583e-17 x_value:  -6.650833e-09\n",
      "Epoch:  500 Loss:  3.048242e-23 x_value:  -5.521089e-12\n",
      "Epoch:  600 Loss:  2.092928e-26 x_value:  1.4466955e-13\n",
      "Epoch:  700 Loss:  9.697628e-31 x_value:  9.847654e-16\n",
      "Epoch:  800 Loss:  3.4687296e-36 x_value:  1.8624526e-18\n",
      "Epoch:  900 Loss:  0.0 x_value:  -1.5366877e-20\n",
      "\n",
      "\n",
      "Best Epoch:  826 Best x_value:  1.809685e-20 Smallest Loss:  0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "x = tf.Variable(10.0, trainable=True)\n",
    "# Định nghĩa hàm loss\n",
    "loss = tf.square(x)\n",
    "\n",
    "# Định nghĩa optimizer là Gradient Descent\n",
    "optimizer = tf.compat.v1.train.MomentumOptimizer (learning_rate=learning_rate,\n",
    "                                                  momentum=0.9)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "values = []\n",
    "smallest_loss = float('inf')  # set to infinity initially\n",
    "best_epoch = -1\n",
    "best_x_value = None\n",
    "\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    for epoch in range(n_epochs):\n",
    "        # rất bực ở chỗ là mỗi lần chạy thì nó phải sess.run\n",
    "        _, loss_value, x_value = sess.run([training_op, loss, x])\n",
    "        if loss_value < smallest_loss:\n",
    "            smallest_loss = loss_value\n",
    "            best_epoch = epoch\n",
    "            best_x_value = x_value\n",
    "        values.append((epoch, loss_value, x_value))\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch: \", epoch, \"Loss: \", loss_value, \"x_value: \", x_value)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"Best Epoch: \", best_epoch, \"Best x_value: \", best_x_value, \"Smallest Loss: \", smallest_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Feeding data cho thuật toán. Chúng ta có thể dùng placeholder để feed data cho thuật toán. Placeholder là một node mà nó sẽ không tính toán được. Nó chỉ là một node mà chúng ta có thể feed data vào. Chúng ta có thể feed data vào placeholder bằng cách sử dụng feed_dict. Feed_dict là một dictionary mà key là placeholdeBr và value là data mà chúng ta muốn feed vào."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "A = tf.compat.v1.placeholder(tf.float32, shape=(None, 3))\n",
    "B = A + 5"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T14:21:10.095115746Z",
     "start_time": "2023-08-20T14:21:10.089657207Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[135], line 30\u001B[0m\n\u001B[1;32m     26\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39moptimizers\u001B[38;5;241m.\u001B[39mSGD(learning_rate\u001B[38;5;241m=\u001B[39mlearning_rate),\n\u001B[1;32m     27\u001B[0m               loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmean_squared_error\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     29\u001B[0m \u001B[38;5;66;03m# Huấn luyện mô hình sử dụng Mini-Batch Gradient Descent\u001B[39;00m\n\u001B[0;32m---> 30\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/LearningTensorflow/venv/lib/python3.11/site-packages/keras/src/engine/training_v1.py:856\u001B[0m, in \u001B[0;36mModel.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001B[0m\n\u001B[1;32m    853\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_call_args(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfit\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    855\u001B[0m func \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_training_loop(x)\n\u001B[0;32m--> 856\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    857\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    858\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    859\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    860\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    861\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    862\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    863\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    864\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_split\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_split\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    865\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    866\u001B[0m \u001B[43m    \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshuffle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    867\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclass_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclass_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    868\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    869\u001B[0m \u001B[43m    \u001B[49m\u001B[43minitial_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minitial_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    870\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteps_per_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msteps_per_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    871\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    872\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_freq\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_freq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    873\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_queue_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_queue_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    874\u001B[0m \u001B[43m    \u001B[49m\u001B[43mworkers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mworkers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    875\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_multiprocessing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_multiprocessing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    876\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/LearningTensorflow/venv/lib/python3.11/site-packages/keras/src/engine/training_generator_v1.py:882\u001B[0m, in \u001B[0;36mGeneratorLikeTrainingLoop.fit\u001B[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001B[0m\n\u001B[1;32m    876\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m validation_steps:\n\u001B[1;32m    877\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    878\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`validation_steps` should not be specified if \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    879\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`validation_data` is None.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    880\u001B[0m         )\n\u001B[0;32m--> 882\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_generator\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    883\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    884\u001B[0m \u001B[43m    \u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weights\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    885\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteps_per_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msteps_per_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    886\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    887\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    888\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    889\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    890\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    891\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    892\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_freq\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_freq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    893\u001B[0m \u001B[43m    \u001B[49m\u001B[43mworkers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    894\u001B[0m \u001B[43m    \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshuffle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    895\u001B[0m \u001B[43m    \u001B[49m\u001B[43minitial_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minitial_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    896\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteps_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msteps_per_epoch\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    897\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/LearningTensorflow/venv/lib/python3.11/site-packages/keras/src/engine/training_generator_v1.py:282\u001B[0m, in \u001B[0;36mmodel_iteration\u001B[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001B[0m\n\u001B[1;32m    279\u001B[0m callbacks\u001B[38;5;241m.\u001B[39m_call_batch_hook(mode, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbegin\u001B[39m\u001B[38;5;124m\"\u001B[39m, step, batch_logs)\n\u001B[1;32m    281\u001B[0m is_deferred \u001B[38;5;241m=\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m model\u001B[38;5;241m.\u001B[39m_is_compiled\n\u001B[0;32m--> 282\u001B[0m batch_outs \u001B[38;5;241m=\u001B[39m \u001B[43mbatch_function\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mbatch_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    283\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(batch_outs, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m    284\u001B[0m     batch_outs \u001B[38;5;241m=\u001B[39m [batch_outs]\n",
      "File \u001B[0;32m~/Documents/LearningTensorflow/venv/lib/python3.11/site-packages/keras/src/engine/training_v1.py:1159\u001B[0m, in \u001B[0;36mModel.train_on_batch\u001B[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001B[0m\n\u001B[1;32m   1154\u001B[0m \u001B[38;5;66;03m# If `self._distribution_strategy` is True, then we are in a replica\u001B[39;00m\n\u001B[1;32m   1155\u001B[0m \u001B[38;5;66;03m# context at this point because of the check above.  `train_on_batch` is\u001B[39;00m\n\u001B[1;32m   1156\u001B[0m \u001B[38;5;66;03m# being run for each replica by `self._distribution_strategy` and the\u001B[39;00m\n\u001B[1;32m   1157\u001B[0m \u001B[38;5;66;03m# same code path as Eager is expected to be taken.\u001B[39;00m\n\u001B[1;32m   1158\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrun_eagerly \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_distribution_strategy:\n\u001B[0;32m-> 1159\u001B[0m     output_dict \u001B[38;5;241m=\u001B[39m \u001B[43mtraining_eager_v1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_on_batch\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1160\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1161\u001B[0m \u001B[43m        \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1162\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1163\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1164\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_loss_metrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_output_loss_metrics\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1165\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1166\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1167\u001B[0m         output_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtotal_loss\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1168\u001B[0m         \u001B[38;5;241m+\u001B[39m output_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_losses\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1169\u001B[0m         \u001B[38;5;241m+\u001B[39m output_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetrics\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1170\u001B[0m     )\n\u001B[1;32m   1171\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m [_non_none_constant_value(v) \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m outputs]\n",
      "File \u001B[0;32m~/Documents/LearningTensorflow/venv/lib/python3.11/site-packages/keras/src/engine/training_eager_v1.py:342\u001B[0m, in \u001B[0;36mtrain_on_batch\u001B[0;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001B[0m\n\u001B[1;32m    324\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Calculates the loss and gradient updates for one input batch.\u001B[39;00m\n\u001B[1;32m    325\u001B[0m \n\u001B[1;32m    326\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    339\u001B[0m \u001B[38;5;124;03m      'metrics': list of tensors for metric specified.\u001B[39;00m\n\u001B[1;32m    340\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    341\u001B[0m inputs \u001B[38;5;241m=\u001B[39m training_utils_v1\u001B[38;5;241m.\u001B[39mcast_to_model_input_dtypes(inputs, model)\n\u001B[0;32m--> 342\u001B[0m outs, total_loss, output_losses, masks \u001B[38;5;241m=\u001B[39m \u001B[43m_process_single_batch\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    343\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    344\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    345\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    346\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    347\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_loss_metrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_loss_metrics\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    349\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    350\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(outs, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m    351\u001B[0m     outs \u001B[38;5;241m=\u001B[39m [outs]\n",
      "File \u001B[0;32m~/Documents/LearningTensorflow/venv/lib/python3.11/site-packages/keras/src/engine/training_eager_v1.py:278\u001B[0m, in \u001B[0;36m_process_single_batch\u001B[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001B[0m\n\u001B[1;32m    248\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_process_single_batch\u001B[39m(\n\u001B[1;32m    249\u001B[0m     model,\n\u001B[1;32m    250\u001B[0m     inputs,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    254\u001B[0m     training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    255\u001B[0m ):\n\u001B[1;32m    256\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Calculate the loss and gradient for one input batch.\u001B[39;00m\n\u001B[1;32m    257\u001B[0m \n\u001B[1;32m    258\u001B[0m \u001B[38;5;124;03m       The model weights are updated if training is set to True.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    276\u001B[0m \u001B[38;5;124;03m        ValueError: If the model has no loss to optimize.\u001B[39;00m\n\u001B[1;32m    277\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 278\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mwith\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbackend\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meager_learning_phase_scope\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    279\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\n\u001B[1;32m    280\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining_utils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mRespectCompiledTrainableState\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m    281\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mwith\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mGradientTape\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mas\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtape\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m    282\u001B[0m \u001B[43m            \u001B[49m\u001B[43mouts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtotal_loss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_losses\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmasks\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m_model_loss\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    283\u001B[0m \u001B[43m                \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    284\u001B[0m \u001B[43m                \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    288\u001B[0m \u001B[43m                \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    289\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/python3.11/contextlib.py:137\u001B[0m, in \u001B[0;36m_GeneratorContextManager.__enter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    135\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkwds, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunc\n\u001B[1;32m    136\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 137\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgen)\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m    139\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgenerator didn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt yield\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/LearningTensorflow/venv/lib/python3.11/site-packages/keras/src/backend.py:603\u001B[0m, in \u001B[0;36meager_learning_phase_scope\u001B[0;34m(value)\u001B[0m\n\u001B[1;32m    601\u001B[0m \u001B[38;5;28;01mglobal\u001B[39;00m _GRAPH_LEARNING_PHASES\n\u001B[1;32m    602\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m value \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m}\n\u001B[0;32m--> 603\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mcompat\u001B[38;5;241m.\u001B[39mv1\u001B[38;5;241m.\u001B[39mexecuting_eagerly_outside_functions()\n\u001B[1;32m    604\u001B[0m global_learning_phase_was_set \u001B[38;5;241m=\u001B[39m global_learning_phase_is_set()\n\u001B[1;32m    605\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m global_learning_phase_was_set:\n",
      "\u001B[0;31mAssertionError\u001B[0m: "
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T14:35:11.990177195Z",
     "start_time": "2023-08-20T14:35:11.924304408Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
